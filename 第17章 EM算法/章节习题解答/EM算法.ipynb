{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 琴生不等式：设$f$为凸函数，${X}$为随机变量，那么$\\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X])$，是否正确？ A. 正确 B. 错误\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. EM算法中，E步骤是在构造对数似然的一个（良好）下界，然后M步骤是在优化此下界。这个说法是否正确？ A. 正确 B. 错误"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 以下关于混合高斯模型的说法不正确的是：   \n",
    "A. GMM可以拟合任意分布，在现实生活中也有很多真实案例。   \n",
    "B. 因为高斯分布在全空间中的概率密度都不为零，所以一个样本可以同时属于 GMM 中的多个高斯分布。   \n",
    "C. GMM中每个高斯分布的权重取决于该分布对应数据的数量。   \n",
    "D. 高斯模型是对数据分布的先验假设，因此GMM是参数化模型。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 以下关于EM算法的说法不正确的是：   \n",
    "A. EM算法是一类优化算法的统称，不只可以用来求解GMM的参数。   \n",
    "B. 优化目标函数的性质对EM算法的收敛性没有影响。   \n",
    "C. 在每个EM步骤后，当前参数计算出的似然不可能降低。   \n",
    "D. EM算法的本质是坐标上升，求解SVM所用的SMO也属于这一类算法。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 在17.2节的代码中，将k均值算法改为k-means++算法，观察拟合效果的变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 试使用本章代码的`_calc_log_likelihood`函数，观察EM算法每一轮迭代的对数似然值的变化，验证EM算法关于对数似然的单增优化性质和收敛性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
