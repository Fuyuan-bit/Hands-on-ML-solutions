{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 以下关于集成学习的说法不正确的是：   \n",
    "   A. 在集成学习中，我们可以为数据空间的不同区域使用不同的预测模型，再将预测结果进行组合。   \n",
    "   B. 一组预测模型可以有多种方式进行集成学习。   \n",
    "   C. 有效的集成学习需要集合中的模型具有单一性，最好将同一类型的预测模型结合起来。   \n",
    "   D. 训练集成模型时，单个模型的参数不会随之更新。  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：  \n",
    "A. 正确。数据空间的不同部分可能有比较大差别的模式，在不同区域学习不同的预测模型再组合起来就可以预测这个数据空间的数据了。  \n",
    "B. 正确。一组预测模型可以用不同的方法集成达到不同的效果，比如Bagging，Boosting，Stacking等。  \n",
    "C. 错误。集成学习就是集合不同结构类型的模型才能获得更好的性能的。  \n",
    "D. 正确。单个模型的参数是训练单个模型的时候确定好。集成学习模型训练是将训练好的模型结合起来。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   2. 以下关于提升算法的说法正确的是：   \n",
    "A. AdaBoost算法中，$err$绝对值越小的模型权重绝对值越大，在集成模型中占有主导地位。   \n",
    "B. AdaBoost算法中，需要按照之前学习器的结果对训练数据进行加权采样。   \n",
    "C. GBDT算法用到了“梯度反方向是函数值下降最快方向”的思想。   \n",
    "D. GBDT的正则化约束只考虑了叶节点的数目  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：  \n",
    "A. 正确。AdaBoost是通过错误率来确定每个学习器的权重的，错误率越小权重就越大，就在模型中起到比较重要的影响。  \n",
    "B. 正确。AdaBoost会给训练数据加权进行调整，使得前一轮的错误样本在下一轮更加被关注。  \n",
    "C. 正确。这是GBDT的核心思想。将梯度和决策树结合，每次迭代拟合决策树模型进行更新。  \n",
    "D. 错误。GBDT的正则化约束还考虑了树的深度，叶结点的输出值等，综合约束来降低过拟合。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 由基础学习器提取特征后再供给元学习器进一步学习，这一特征提取的思想在前面哪些章节也出现过？为什么合适的特征提取往往能提升算法的表现？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：特征提取可以从大量的特征洪提取合适的特征，减少特征的维度和噪声，找到更加能拟合数据模式的重要特征，达到更好的泛化效果。而且特征提取可以降低计算的复杂度，消除一些不必要的关系影响。  \n",
    "特征提取在许多地方都出现过，比如CNN中的对图像数据的提取（池化），RNN中对序列中潜在关系的提取（GRU），SVM中的特征映射，决策树中的对数据的分割提取等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 基于本章代码，尝试非线性的元分类器，如神经网络和决策树，观察原始数据拼接到新数据上的模型预测性能的改变。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 在提升算法中，弱学习器的数量越多，元学习器的效果是否一定越好？调整AdaBoost和GBDT代码中弱学习器的数量，验证你的想法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 基于xgboost库，对于本章涉及的回归任务，调试树的数量上限、每棵树的深度上限、学习率，观察其训练模型性能的改变，讨论是大量较浅的树组成的GBDT模型更强，还是少量的较深的树组成的GBDT模型更强。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
